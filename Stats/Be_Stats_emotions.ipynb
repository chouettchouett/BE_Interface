{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# Téléchargements NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Chargement des données\n",
        "df = pd.read_csv(\"friends_dialogues.csv\")\n",
        "main_characters = ['Chandler', 'Joey', 'Monica', 'Phoebe', 'Rachel', 'Ross']\n",
        "df = df[df[\"character\"].isin(main_characters)]\n",
        "\n",
        "# Modèle DistilBERT pour l'analyse des émotions\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "emotion_pipeline = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\", device=device)\n",
        "\n",
        "# Initialisation des outils\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Fonctions\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def analyze_emotion_batch(texts, batch_size=32):\n",
        "    results = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        results.extend(emotion_pipeline(batch))\n",
        "    return results\n",
        "\n",
        "# Analyse des émotions\n",
        "emotion_results = analyze_emotion_batch(df[\"line\"].tolist())\n",
        "emotions = [{\"emotion\": result[\"label\"], \"score\": result[\"score\"]} for result in emotion_results]\n",
        "\n",
        "# Ajout des émotions au DataFrame\n",
        "df[\"emotion\"] = [emotion[\"emotion\"] for emotion in emotions]\n",
        "df[\"score_emotion\"] = [emotion[\"score\"] for emotion in emotions]\n",
        "\n",
        "# Extraction des tics de langage\n",
        "def extract_tics(dialogues):\n",
        "    tokens_dialogues = [preprocess_text(dialogue) for dialogue in dialogues]\n",
        "    all_ngrams = []\n",
        "    for tokens in tokens_dialogues:\n",
        "        all_ngrams.extend(list(ngrams(tokens, 3)))  # trigrammes\n",
        "    return Counter(all_ngrams).most_common(1)\n",
        "\n",
        "# Extraction des tics de langage et des phrases préférées\n",
        "phrases_preferees = {}\n",
        "tics_par_personnage = {}\n",
        "\n",
        "for personnage in main_characters:\n",
        "    dialogues_personnage = df[df[\"character\"] == personnage][\"line\"].tolist()\n",
        "    tics = extract_tics(dialogues_personnage)\n",
        "    tics_par_personnage[personnage] = tics\n",
        "\n",
        "    if tics:\n",
        "        tic_mots = \" \".join(tics[0][0])\n",
        "        phrase_preferee = \"\"\n",
        "        for dialogue in dialogues_personnage:\n",
        "            if tic_mots in dialogue.lower():\n",
        "                phrase_preferee = dialogue\n",
        "                break\n",
        "        phrases_preferees[personnage] = phrase_preferee\n",
        "    else:\n",
        "        phrases_preferees[personnage] = \"N/A\"\n",
        "\n",
        "# Ajout des phrases préférées et des tics au DataFrame\n",
        "df[\"phrase_preferee\"] = df[\"character\"].apply(lambda x: phrases_preferees.get(x, \"N/A\"))\n",
        "df[\"tics_langage\"] = df[\"character\"].apply(lambda x: \" \".join([\" \".join(tic) for tic in tics_par_personnage.get(x, [])[0][0]]) if tics_par_personnage.get(x) else \"N/A\")\n",
        "\n",
        "# Création du DataFrame pour les phrases préférées et les tics\n",
        "phrases_tics_df = pd.DataFrame({\n",
        "    \"Personnage\": main_characters,\n",
        "    \"Phrase préférée\": [phrases_preferees[personnage] for personnage in main_characters],\n",
        "    \"Tics de langage\": [\" \".join([\" \".join(tic) for tic in tics_par_personnage[personnage][0][0]]) if tics_par_personnage[personnage] else \"N/A\" for personnage in main_characters]\n",
        "})\n",
        "\n",
        "# Écriture dans les fichiers CSV\n",
        "df.drop(columns=[\"phrase_preferee\", \"tics_langage\"], inplace=True) #Suppression des colonnes\n",
        "df.to_csv(\"friends_dialogues_emotions.csv\", index=False)\n",
        "phrases_tics_df.to_csv(\"friends_dialogues_phrases_tics.csv\", index=False)\n",
        "\n",
        "print(\"Résultats écrits dans friends_dialogues_emotions.csv et friends_dialogues_phrases_tics.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P42xogdrYNRP",
        "outputId": "06fbc5f1-a354-44ec-b5d5-549f3436cd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultats écrits dans friends_dialogues_emotions.csv et friends_dialogues_phrases_tics.csv\n"
          ]
        }
      ]
    }
  ]
}