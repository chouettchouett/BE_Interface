# -*- coding: utf-8 -*-
"""Analyse_Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQKXCo1_oKEMEF1Atv-UVrKqzztG1auX
"""
# Cantalejo Jorian

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.util import ngrams
from collections import Counter
from transformers import pipeline
import re
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from wordcloud import WordCloud

# Téléchargements NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Chargement des données
df = pd.read_csv("friends_dialogues_final.csv")
main_characters = ['Chandler', 'Joey', 'Monica', 'Phoebe', 'Rachel', 'Ross']
df = df[df["character"].isin(main_characters)]

# Modèle DistilBERT pour l'analyse des émotions
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
emotion_pipeline = pipeline("text-classification", model="bhadresh-savani/distilbert-base-uncased-emotion", device=device)

# Initialisation des outils
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Fonctions
def preprocess_text(text):
    text = re.sub(r'[^\w\s]', '', text).lower()
    tokens = word_tokenize(text)
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]
    return lemmatized_tokens

def analyze_emotion_batch(texts, batch_size=32):
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        results.extend(emotion_pipeline(batch))
    return results

# Analyse des émotions
emotion_results = analyze_emotion_batch(df["line"].tolist())
emotions = [{"emotion": result["label"], "score": result["score"]} for result in emotion_results]

# Ajout des émotions au DataFrame
df["emotion"] = [emotion["emotion"] for emotion in emotions]
df["score_emotion"] = [emotion["score"] for emotion in emotions]

# Extraction des tics de langage
def extract_tics(dialogues, n=4): #N-grammes de 4 mots
    tokens_dialogues = [preprocess_text(dialogue) for dialogue in dialogues]
    all_ngrams = []
    for tokens in tokens_dialogues:
        all_ngrams.extend(list(ngrams(tokens, n)))
    return Counter(all_ngrams).most_common(1)

# Extraction des informations par saison et personnage
saison_personnage_data = []

for saison in df["season"].unique():
    for personnage in main_characters:
        df_saison_personnage = df[(df["season"] == saison) & (df["character"] == personnage)]
        dialogues_saison_personnage = df_saison_personnage["line"].tolist()

        # Nombre de phrases
        nombre_phrases = len(dialogues_saison_personnage)

        # Sentiments les plus récurrents
        sentiments_saison_personnage = df_saison_personnage["emotion"].tolist()
        sentiments_recurrents = Counter(sentiments_saison_personnage).most_common(3)

        # Tics de langage
        tics = extract_tics(dialogues_saison_personnage)
        tic_mots = " ".join(tics[0][0]) if tics else "N/A"

        # Phrase préférée (la plus récurrente)
        phrase_preferee = "N/A"
        phrase_counts = Counter(dialogues_saison_personnage)
        if phrase_counts:
            filtered_phrases = [
                phrase for phrase in dialogues_saison_personnage
                if len(phrase.split()) >= 4 and
                not re.match(r"^(hey|hi|what's|what is|are you)\b", phrase.lower())
            ]
            if filtered_phrases:
                phrase_counts_filtered = Counter(filtered_phrases)
                if phrase_counts_filtered:
                    phrase_preferee = phrase_counts_filtered.most_common(1)[0][0]

        saison_personnage_data.append({
            "Saison": saison,
            "Personnage": personnage,
            "Nombre de phrases": nombre_phrases,
            "Sentiments récurrents": ", ".join([f"{emotion} ({count})" for emotion, count in sentiments_recurrents]),
            "Tic de langage": tic_mots,
            "Phrase préférée": phrase_preferee
        })

# Création du DataFrame saison_personnage
saison_personnage_df = pd.DataFrame(saison_personnage_data)

# Fusion des DataFrames
df = pd.merge(df, saison_personnage_df, left_on=["season", "character"], right_on=["Saison", "Personnage"])

# Convertir la colonne "Saison" en catégorie ordonnée
df["Saison"] = pd.Categorical(df["Saison"], categories=sorted(df["Saison"].unique()), ordered=True)

# Écriture dans un fichier CSV
df.to_csv("friends_dialogues_analysis.csv", index=False)

# Ordre des sentiments pour l'affichage
emotion_order = ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise']

# Dictionnaire de couleurs pour les sentiments
emotion_colors_fixed = {
    'joy': 'green',
    'sadness': 'blue',
    'anger': 'red',
    'fear': 'orange',
    'love': 'purple',
    'surprise': 'brown'
}

# 1. Évolution des sentiments par personnage (séparé par personnage)
plt.figure(figsize=(18, 12))
for i, personnage in enumerate(main_characters):
    plt.subplot(2, 3, i + 1)
    df_personnage = df[df["Personnage"] == personnage]
    for emotion in emotion_order:
        df_emotion = df_personnage[df_personnage["emotion"] == emotion]
        sns.lineplot(x="Saison", y="score_emotion", data=df_emotion, color=emotion_colors_fixed[emotion], errorbar=None, label=emotion.capitalize())
    plt.title(f"Évolution des sentiments - {personnage}")
    plt.xlabel("Saison")
    plt.ylabel("Score moyen du sentiment")
    plt.legend(title='Sentiment', loc='upper right')
plt.tight_layout()
plt.show()

# 2. Sentiments les plus récurrents par personnage et saison (séparé par personnage)
plt.figure(figsize=(18, 12))
for i, personnage in enumerate(main_characters):
    plt.subplot(2, 3, i + 1)
    df_personnage = df[df["Personnage"] == personnage]
    sns.countplot(x="Saison", hue="emotion", data=df_personnage, palette=emotion_colors_fixed, hue_order=emotion_order) # Utilisation du dictionnaire fixe et ordre
    plt.title(f"Sentiments récurrents - {personnage}")
    plt.legend(title='Sentiment', loc='upper right', labels=[emotion.capitalize() for emotion in emotion_order]) # Ordre de la légende
plt.tight_layout()
plt.show()

# 3. Nombre de phrases par personnage et saison (séparé par personnage)
plt.figure(figsize=(18, 12))
for i, personnage in enumerate(main_characters):
    plt.subplot(2, 3, i + 1)
    df_personnage = df[df["Personnage"] == personnage]
    sns.barplot(x="Saison", y="Nombre de phrases", data=df_personnage)
    plt.title(f"Nombre de phrases - {personnage}")
plt.tight_layout()
plt.show()

# 4. Tics de langage par personnage (séparé par personnage)
plt.figure(figsize=(18, 12))
for i, personnage in enumerate(main_characters):
    plt.subplot(2, 3, i + 1)
    df_personnage = df[df["Personnage"] == personnage]
    top_tics = df_personnage["Tic de langage"].value_counts().nlargest(5).index #Top 5 tics
    df_top_tics = df_personnage[df_personnage["Tic de langage"].isin(top_tics)]
    sns.countplot(y="Tic de langage", data=df_top_tics, order=top_tics)
    plt.title(f"Tics de langage - {personnage}")
plt.tight_layout()
plt.show()

# 5. Nuages de mots par personnage
plt.figure(figsize=(18, 12))
for i, personnage in enumerate(main_characters):
    plt.subplot(2, 3, i + 1)
    df_personnage = df[df["Personnage"] == personnage]
    text = " ".join(df_personnage["line"].tolist())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Nuage de mots - {personnage}")
plt.tight_layout()
plt.show()

# 6. Diversité lexicale par personnage
plt.figure(figsize=(10, 6))
lexical_diversity = []
for personnage in main_characters:
    df_personnage = df[df["Personnage"] == personnage]
    tokens = " ".join(df_personnage["line"].tolist()).split()
    unique_tokens = len(set(tokens))
    total_tokens = len(tokens)
    lexical_diversity.append(unique_tokens / total_tokens)
sns.barplot(x=main_characters, y=lexical_diversity)
plt.title("Diversité lexicale par personnage")
plt.ylabel("Diversité lexicale")
plt.show()

# 7. Longueur moyenne des phrases par personnage
plt.figure(figsize=(10, 6))
sentence_length = []
for personnage in main_characters:
    df_personnage = df[df["Personnage"] == personnage]
    sentence_length.append(df_personnage["line"].apply(lambda x: len(x.split())).mean())
sns.barplot(x=main_characters, y=sentence_length)
plt.title("Nombre moyen de mots par phrase")
plt.ylabel("Nombre moyen de mots des phrases")
plt.show()

print("Résultats écrits dans friends_dialogues_analysis.csv et diagrammes créés.")
